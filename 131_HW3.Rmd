---
title: "131_HW3"
author: "Zack Reardon"
output: github_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache.extra = rand_seed)
```

```{r, message=FALSE}
library(tidyverse)
library(tidymodels)
library(corrr)
library(discrim)
library(klaR)
library(poissonreg)
library(pROC)
tidymodels_prefer()

titanic <- read_csv("/Users/zackreardon/Downloads/homework-3/data/titanic.csv")

# converting to factors
titanic$survived <- as.factor(titanic$survived)
titanic$pclass <- as.factor(titanic$pclass)
```

Question 1.

```{r}
set.seed(100)

titanic_split <- initial_split(titanic, prop = 0.70, strata = survived)

titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)

tr <- nrow(titanic_train) # determine number of observations
tt <- nrow(titanic_test) # determine number of observations

print(tr/(tr+tt))

titanic_train # look at training data
```
There appears to be some missing data regarding the age of some passengers as well as the cabin number. It is a good idea to use stratified sampling for this data since the purpose of the model is to predict survival categorizations. By stratifying the data on the survival variable, there exists a similar proportion of each categorization in the training and testing sets which allows for a more effective model.

Question 2.

```{r}
ggplot(titanic_train, aes(x=survived)) + geom_bar()
```

The distribution of the outcome variable in the training set indicates that between 3/2 and double the amount of people who survived did not survive.

Question 3.

```{r}
cor_titanic <- titanic_train %>%
  select(is.numeric) %>%
  correlate()
rplot(cor_titanic)
```

Age and siblings/spouses are negatively correlated as well as parents/children and age. Parents/children and siblings/spouses, fare and siblings/spouses, and fare and parents/children are positively correlated.

Question 4.

```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>%
  step_impute_linear(age, impute_with = imp_vars(all_predictors())) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ starts_with("sex"):fare) %>%
  step_interact(~ age:fare)
```

Question 5.

```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

log_fit <- fit(log_wkflow, titanic_train)
```

Question 6.

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>%
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>%
  add_recipe(titanic_recipe)

lda_fit <- fit(lda_wkflow, titanic_train)
```

Question 7.

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)

qda_fit <- fit(qda_wkflow, titanic_train)
```

Question 8.

```{r}
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>%
  set_args(usekernel = FALSE)

nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(titanic_recipe)

nb_fit <- fit(nb_wkflow, titanic_train)
```

Question 9.

```{r, warning=FALSE}
log <- predict(log_fit, new_data = titanic_train, type = "prob")
lda <- predict(lda_fit, new_data = titanic_train, type = "prob")
qda <- predict(qda_fit, new_data = titanic_train, type = "prob")
nb <- predict(nb_fit, new_data = titanic_train, type = "prob")
bind_cols(log, lda, qda, nb, .name_repair="minimal")

log <- augment(log_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
lda <- augment(lda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
qda <- augment(qda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
nb <- augment(nb_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)

accuracies <- c(log$.estimate, lda$.estimate, 
                qda$.estimate, nb$.estimate)
models <- c("Logistic Regression", "LDA", "QDA", "Naive Bayes")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```
The logistic regression had the highest accuracy on the training data.

Question 10.

```{r}
predict(log_fit, new_data = titanic_test, type = "prob")

# report accuracy of model on testing data
augment(log_fit, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)

augment(log_fit, new_data = titanic_test) %>%
  conf_mat(truth = survived, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

augment(log_fit, new_data = titanic_test) %>%
  roc_curve(survived, .pred_No) %>%
  autoplot()

# calculate AUC
roc <- augment(log_fit, new_data = titanic_test) %>%
  roc(survived, .pred_No)
auc(roc)
```

The model performed fairly well. The training accuracy was 0.806 while the test accuracy was 0.840 which interestingly represents an increase. The accuracy for the test may have been higher since there where fewer data points and they could have leant themselves well to the model. If the amount of data was higher in the test set, I would expect the accuracy to be closer to that of the training data.